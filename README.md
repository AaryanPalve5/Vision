#  Door & Window Detection in Architectural Blueprints

**Final Submission for Palcode.ai Internship Assignment**

âœ… Manual Labeling â†’ âœ… Augmentation â†’ âœ… YOLOv8 â†’ âœ… ONNX â†’ âœ… API â†’ âœ… Deployed

---

## ðŸš€ Project Overview

This project detects `door` and `window` objects in **construction blueprint images**.

* Labeled with **Roboflow**
  ðŸ‘‰ [Roboflow Project Link](https://universe.roboflow.com/asp-2t4fy/doors-and-windows-cgn3z/dataset/2)
* Augmented in Roboflow
* Trained **YOLOv8**
* Exported to **ONNX** with NMS
* Flask app with:

  * Web UI (upload images + visualize detections)
  * `/detect` API returning JSON
* Deployed on **Render**
  ðŸ‘‰ [https://vision-pylv.onrender.com](https://vision-pylv.onrender.com)

---

## ðŸ“‚ Repository Structure

```
â”œâ”€â”€ Doors and Windows/          # images/ + labels/ (organized)
â”œâ”€â”€ Proof of work/              # Screenshots, labeling, training proofs
â”‚   â”œâ”€â”€ Screenshot 2025-05-30 at 3.32.58â€¯PM.png  # Labeling screenshot
â”‚   â”œâ”€â”€ Results post augmentation.png           # Training results / loss graph
â”œâ”€â”€ Raw dataset/                # Original images
â”œâ”€â”€ static/results/             # Output images with bounding boxes
â”œâ”€â”€ templates/index.html        # Web UI template
â”œâ”€â”€ uploads/                    # Temporary upload folder
â”œâ”€â”€ 16.png                      # Test image
â”œâ”€â”€ Doors and Windows.zip       # ZIP of images + labels
â”œâ”€â”€ README.md                   # This file
â”œâ”€â”€ app.py                      # Flask app with Web UI + /detect API
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ test.py                     # (optional test script)
â”œâ”€â”€ vision2.onnx                # Optimized ONNX model
â””â”€â”€ x.py                        # (helper/test script)
```

---

## ðŸ·ï¸ Classes

```
door
window
```

---

## ðŸ› ï¸ Setup Instructions

### 1ï¸âƒ£ Clone Repository

```bash
git clone https://github.com/yourusername/blueprint-door-window-detection.git
cd blueprint-door-window-detection
```

### 2ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

### 3ï¸âƒ£ Run Locally

```bash
python app.py
```

Web UI: [http://localhost:8000](http://localhost:8000)
API: [http://localhost:8000/detect](http://localhost:8000/detect)

---

## ðŸ–¼ï¸ API Usage

### POST `/detect`

* Accepts: PNG or JPG image
* Returns: JSON with bounding boxes

### Example `curl`:

```bash
curl -X POST https://vision-pylv.onrender.com/detect \
  -F "file=@16.png"
```

### Example Response:

```json
{
  "detections": [
    {
      "label": "door",
      "confidence": 0.91,
      "bbox": [x1, y1, x2, y2]
    },
    {
      "label": "window",
      "confidence": 0.84,
      "bbox": [x1, y1, x2, y2]
    }
  ]
}
```

---

## âš™ï¸ Pipeline

| Step         | Tool                 |
| ------------ | -------------------- |
| Labeling     | Roboflow             |
| Augmentation | Roboflow             |
| Training     | YOLOv8               |
| Optimization | ONNX export with NMS |
| Inference    | ONNXRuntime          |
| API          | Flask                |
| Deployment   | Render               |

---

## ðŸ“¸ Proof of Work

### 1ï¸âƒ£ Labeling Screenshot

ðŸ“ `Proof of work/Screenshot 2025-05-30 at 3.32.58â€¯PM.png`

![Labeling](Proof%20of%20work/Screenshot%202025-05-30%20at%203.32.58%E2%80%AFPM.png)

---

### 2ï¸âƒ£ Training Screenshot / Loss Graph

ðŸ“ `Proof of work/Results post augmentation.png`

![Training](Proof%20of%20work/Results%20post%20augmentation.png)

---

### 3ï¸âƒ£ API Response Example

Example shown on deployed API: [https://vision-pylv.onrender.com](https://vision-pylv.onrender.com)



---

## ðŸ”— Final Submission Links

âœ… GitHub Repo: [https://github.com/yourusername/blueprint-door-window-detection](https://github.com/yourusername/blueprint-door-window-detection)
âœ… Public API URL (Render): [https://vision-pylv.onrender.com](https://vision-pylv.onrender.com)
âœ… Loom Video: [https://loom.com/share/your-video-link](https://loom.com/share/your-video-link)

---

## Notes

* ONNX runtime ensures fast inference
* Threshold: `confidence > 0.4`
* Handles **non-labeled blueprint sheets** gracefully
* Web UI and API both ready for testing

---

